"""
å·¥å…·æ¨¡å—ï¼Œæä¾›JSONè§£æã€SSEæ•°æ®è½¬æ¢ã€æœç´¢è¯·æ±‚å¤„ç†ç­‰åŠŸèƒ½ã€‚
"""

import ast
import json
import re
import sys
import time
import traceback
import logging
from pathlib import Path
from typing import Dict, List, Union

from bs4 import BeautifulSoup
import docx
import fitz
from openai import OpenAI
import openpyxl
from pydocx import PyDocX
import requests
import xlrd

# æ·»åŠ æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from app.search.models import SearchRequest,QueryKeys
from app.utils.config import AVAILABLE_EXTENSIONS

def response2json(text: str, mode: str = "json_str") -> Union[Dict, List, None]:
    """
    ä»å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹ æˆ–è€… ç¬¬ä¸€ä¸ª [ å’Œ æœ€åä¸€ä¸ª ] ä¹‹é—´çš„å†…å®¹,
    å¹¶å°è¯•å°†å…¶è§£æä¸º JSON å¯¹è±¡æˆ–åˆ—è¡¨ã€‚
    ä¼šæ ¹æ® mode ä¼˜å…ˆå°è¯•è§£æã€‚å¦‚æœé¦–é€‰ç±»å‹æœªæ‰¾åˆ°ï¼Œä¼šå°è¯•å¦ä¸€ç§ç±»å‹ä½œä¸ºå›é€€ã€‚

    å‚æ•°:
        text (str): è¦å¤„ç†çš„è¾“å…¥å­—ç¬¦ä¸²
        mode (str): "json_str" (é»˜è®¤) ä¼˜å…ˆæŸ¥æ‰¾ JSON å¯¹è±¡ ({...}),
                    "json_list" ä¼˜å…ˆæŸ¥æ‰¾ JSON åˆ—è¡¨ ([...])ã€‚

    è¿”å›:
        dict/list/None: æˆåŠŸæ—¶è¿”å›è§£æåçš„ JSON å¯¹è±¡æˆ–åˆ—è¡¨, å¤±è´¥æ—¶è¿”å› None
    """
    if text.rstrip(" ").startswith("<think>"):
        text = text.split("</think>",maxsplit=1)[-1]
    text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
    
    pattern_json_obj = r"({.*})"  # åŒ¹é… JSON å¯¹è±¡: ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }
    pattern_json_list = r"(\[.*\])"  # åŒ¹é… JSON åˆ—è¡¨: ç¬¬ä¸€ä¸ª [ åˆ°æœ€åä¸€ä¸ª ]
    
    match_obj = re.search(pattern_json_obj, text, re.DOTALL)
    match_list = re.search(pattern_json_list, text, re.DOTALL)
    
    string_to_parse = None
    
    if mode == "json_list":
        if match_list:
            string_to_parse = match_list.group(1)
        elif match_obj:  # å›é€€: å¦‚æœåˆ—è¡¨æ¨¡å¼ä¸‹æœªæ‰¾åˆ°åˆ—è¡¨ï¼Œåˆ™å°è¯•å¯¹è±¡
            string_to_parse = match_obj.group(1)
    else:
        if match_obj:
            string_to_parse = match_obj.group(1)
        elif match_list:  # å›é€€: å¦‚æœå¯¹è±¡æ¨¡å¼ä¸‹æœªæ‰¾åˆ°å¯¹è±¡ï¼Œåˆ™å°è¯•åˆ—è¡¨
            string_to_parse = match_list.group(1)
    
    if string_to_parse:
        try:
            parsed_json_content = json.loads(string_to_parse)
            return parsed_json_content
        except json.JSONDecodeError as e:
            logging.error(f"JSON è§£æå¤±è´¥: {e}")
            return None
    else:
        logging.warning("æœªæ‰¾åˆ°åŒ¹é…çš„ JSON æ ¼å¼å†…å®¹")
        return None


def response2list(llm_output: str) -> list[int]:
    """
    ä»ä»»æ„LLMè¾“å‡ºä¸­æå–æ•´æ•°åˆ—è¡¨
    
    Args:
        llm_output (str): å¤§æ¨¡å‹çš„è¾“å‡ºæ–‡æœ¬
        
    Returns:
        list: æå–å‡ºçš„æ•´æ•°åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ—è¡¨æ ¼å¼
    llm_output = str(llm_output)
    list_pattern = r'\[\s*(?:\d+\s*(?:,\s*\d+\s*)*)?]'
    potential_lists = re.findall(list_pattern, llm_output)
    
    valid_lists = []
    for list_str in potential_lists:
        try:
            # ä½¿ç”¨ast.literal_evalå®‰å…¨åœ°è§£æåˆ—è¡¨
            parsed_list = ast.literal_eval(list_str)
            # ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªåˆ—è¡¨å¹¶ä¸”æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æ•´æ•°
            if isinstance(parsed_list, list) and all(isinstance(item, int) for item in parsed_list):
                valid_lists.append((list_str, parsed_list))
        except (SyntaxError, ValueError):
            continue
    
    if not valid_lists:
        return []
    
    # å¦‚æœåªæ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆåˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if len(valid_lists) == 1:
        return valid_lists[0][1]
    
    # å¦‚æœæœ‰å¤šä¸ªåˆ—è¡¨ï¼Œä¼˜å…ˆé€‰æ‹©æœ€é•¿çš„åˆ—è¡¨
    max_length = max(len(parsed_list) for _, parsed_list in valid_lists)
    longest_lists = [(list_str, parsed_list) for list_str, parsed_list in valid_lists 
                     if len(parsed_list) == max_length]
    
    # å¦‚æœæœ‰å¤šä¸ªç›¸åŒé•¿åº¦çš„åˆ—è¡¨ï¼Œé€‰æ‹©æœ€åå‡ºç°çš„é‚£ä¸ª
    last_list = None
    last_position = -1
    
    for list_str, parsed_list in longest_lists:
        list_pos = llm_output.rfind(list_str)
        if list_pos > last_position:
            last_position = list_pos
            last_list = parsed_list
    
    return last_list if last_list is not None else []


def get_time() -> str:
    """è·å–å½“å‰æ—¶é—´çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
    return str(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))


def sse_create_openai_data(content: str = '', reasoning_content: str = '') -> str:
    """åˆ›å»ºOpenAIæ ¼å¼çš„SSEæ•°æ®"""
    data_structure = {
        "choices": [
            {
                "delta": {
                    "content": str(content),
                    "reasoning_content": str(reasoning_content),
                    "role": "assistant"
                },
                "index": 0
            }
        ],
        "created": 0,  # æˆ–è€… int(time.time())
        "id": int(time.time()),
        "model": "summary_model",
        "service_tier": "default",
        "object": "chat.completion.chunk",
        "usage": None
    }
    json_data = json.dumps(data_structure, ensure_ascii=False)
    return f"data: {json_data}\n\n"


def sse_create_openai_usage_data(usage: dict) -> str:
    """åˆ›å»ºOpenAIæ ¼å¼çš„ä½¿ç”¨ç»Ÿè®¡SSEæ•°æ®"""
    completion_tokens = usage.get("completion_tokens", 0)
    prompt_tokens = usage.get("prompt_tokens", 0)
    total_tokens = usage.get("total_tokens", 0)
    template = f"""data: {{"usage":{{"completion_tokens":{completion_tokens},"prompt_tokens":{prompt_tokens},"total_tokens":{total_tokens}}},"choices":[],"created":{time.time()},"id":"search-{time.time()}","model":"search-model","object":"chat.completion.chunk"}}\n\n"""
    return template


def sse_gemini2openai_data(gemini_sse_data: str) -> str:
    """å°†Geminiçš„SSEæ•°æ®è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
    gemini_sse_data = gemini_sse_data.decode("utf-8")
    if gemini_sse_data.startswith("data: "):
        json_str = gemini_sse_data[6:]
        try:
            json_data = json.loads(json_str)
            content = json_data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
            data_structure = {
                "choices": [
                    {
                        "delta": {
                            "content": content,
                            "role": "assistant"
                        },
                        "index": 0
                    }
                ],
                "created": 0,
                "id": int(time.time()),
                "model": "summary_model", 
                "service_tier": "default",
                "object": "chat.completion.chunk",
                "usage": None
            }
            json_data = json.dumps(data_structure, ensure_ascii=False)
            return f"data: {json_data}\n\n"
        except Exception:
            pass
    else:
        logging.debug(gemini_sse_data)


def text2fc(llm_str: str) -> list:
    """ä»LLMè¾“å‡ºä¸­æå–å‡½æ•°è°ƒç”¨ä¿¡æ¯"""
    class fc:
        def __init__(self, name, arguments):
            self.function = self.function_llm(name, arguments)
            
        class function_llm:
            def __init__(self, name, arguments):
                self.name = name
                self.arguments = arguments
                
    pattern = r"<function_call>(.*?)</function_call>"
    matches = re.findall(pattern, llm_str, re.DOTALL)
    if matches:
        fc_text = matches[0].strip("\n")
        try:
            fc_json = json.loads(fc_text)
            fc_name = fc_json.get('name', None)
            fc_arguments = fc_json.get('arguments', None)
            fc_message = fc(fc_name, fc_arguments)
            return [fc_message]
        except Exception as e:
            logging.error("function callæå–å‡ºé—®é¢˜")
            logging.error(e)
            logging.error(fc_text)


def json2SearchRequests(data: dict) -> SearchRequest:
    """
    å°†JSONæ•°æ®è½¬æ¢ä¸ºSearchRequestå¯¹è±¡åˆ—è¡¨
    
    å‚æ•°:
        data (dict): åŒ…å«æœç´¢è¯·æ±‚ä¿¡æ¯çš„å­—å…¸ï¼Œåº”åŒ…å«'search_purpose'ã€'data'å’Œ'time_page'å­—æ®µ
        
    è¿”å›:
        list[SearchRequest]: SearchRequestå¯¹è±¡åˆ—è¡¨
    """
    search_purpose = data.get('search_purpose', '')
    search_restrictions = data.get('search_restrictions','')
    search_data = data.get('data', [])
    time_page = data.get('time_page', '')
    # éªŒè¯search_dataæ˜¯åˆ—è¡¨
    if not isinstance(search_data, list):
        return []
    
    search_request = SearchRequest(
            query_keys=[QueryKeys(key=r.get('keys', ''),language=r.get('language', 'zh_CN')) for r in search_data],
            time_page=time_page,
            search_purpose=search_purpose,
            search_restrictions=search_restrictions
        )
    
    return search_request


def format_search_plan(plan_info: dict):
    """
    ç¾åŒ–æœç´¢è®¡åˆ’è¾“å‡ºçš„ç”Ÿæˆå™¨å‡½æ•° - äºŒçº§èœå•æ ·å¼
    """
    search_purpose = plan_info.get('search_purpose', 'æœªçŸ¥')
    search_restrictions = plan_info.get('search_restrictions', 'æ— ')
    search_keywords = [item.get('keys', '') for item in plan_info.get('data', [])]
    
    yield f"ğŸ¯ **æœç´¢é¢„æœŸï¼š** {search_purpose}\n"
    yield f"â­• **ç»“æœé™åˆ¶ï¼š** {search_restrictions}\n"
    yield f"ğŸ” **æœç´¢å…³é”®è¯ï¼š**\n"
    
    for keyword in search_keywords:
        yield f"\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0â¤ {keyword}\n"

def format_urls(urls: List[str]):
    """
    ç¾åŒ–URLåˆ—è¡¨è¾“å‡ºçš„ç”Ÿæˆå™¨å‡½æ•°
    """
    if not urls:
        yield "ğŸ“ **æœªæŸ¥çœ‹ä»»ä½•ç½‘é¡µå†…å®¹**\n"
        return
    
    yield f"ğŸŒ **å·²æŸ¥çœ‹ {len(urls)} ä¸ªç½‘é¡µï¼š**\n"
    
    for i, url in enumerate(urls, 1):
        if url:
            display_url = url if len(url) <= 100 else url[:120] + "..."
            yield f"\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0{i}. {display_url}\n"
    
    yield "\n"

DOWNLOAD_FILE_PATH = ROOT_DIR / "tmp_files"
def download_file(url):
    """
    ä»æŒ‡å®š URL ä¸‹è½½æ–‡ä»¶ï¼Œå¹¶ä¿å­˜åˆ° DOWNLOAD_FILE_PATH ç›®å½•ã€‚
    å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºè¯¥ç›®å½•ã€‚
    
    Returns:
        Path|str: æˆåŠŸæ—¶è¿”å›æ–‡ä»¶è·¯å¾„(Pathå¯¹è±¡ï¼Œå¸ƒå°”å€¼ä¸ºTrue)ï¼Œå¤±è´¥æ—¶è¿”å›ç©ºå­—ç¬¦ä¸²(å¸ƒå°”å€¼ä¸ºFalse)
    """
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    
    DOWNLOAD_FILE_PATH.mkdir(parents=True, exist_ok=True)
    
    # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
    file_name = url.split('/')[-1]
    file_extension = Path(file_name).suffix.lower()
    if file_extension not in AVAILABLE_EXTENSIONS:
        return ''
    
    file_path = DOWNLOAD_FILE_PATH / file_name
    
    try:
        # å…ˆå‘é€HEADè¯·æ±‚æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆå¦‚æœæœåŠ¡å™¨æ”¯æŒï¼‰
        head_response = requests.head(url, timeout=10)
        if head_response.status_code == 200:
            content_length = head_response.headers.get('Content-Length')
            if content_length and int(content_length) > MAX_FILE_SIZE:
                logging.warning(
                    f"æ–‡ä»¶è¿‡å¤§: {int(content_length) / (1024*1024):.1f}MB > 10MB"
                )
                return ''
        
        # æµå¼ä¸‹è½½å¹¶é™åˆ¶å¤§å°
        response = requests.get(url, timeout=30, stream=True)
        response.raise_for_status()
        
        downloaded_size = 0
        with open(file_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    downloaded_size += len(chunk)
                    if downloaded_size > MAX_FILE_SIZE:
                        logging.warning(
                            f"ä¸‹è½½è¿‡ç¨‹ä¸­å‘ç°æ–‡ä»¶è¿‡å¤§: {downloaded_size / (1024*1024):.1f}MB > 10MB"
                        )
                        # åˆ é™¤éƒ¨åˆ†ä¸‹è½½çš„æ–‡ä»¶
                        file_path.unlink(missing_ok=True)
                        return ''
                    f.write(chunk)
        
        return file_path  # Pathå¯¹è±¡ï¼Œå¸ƒå°”å€¼ä¸ºTrue
        
    except requests.RequestException as e:
        # ç¡®ä¿æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†æ–‡ä»¶
        if file_path.exists():
            file_path.unlink(missing_ok=True)
        return ''  # ç©ºå­—ç¬¦ä¸²ï¼Œå¸ƒå°”å€¼ä¸ºFalse
    except Exception as e:
        # ç¡®ä¿æ¸…ç†å¯èƒ½çš„éƒ¨åˆ†æ–‡ä»¶  
        if file_path.exists():
            file_path.unlink(missing_ok=True)
        return ''

def extract_text_from_file(file_path):
    """
    æ ¹æ®æ–‡ä»¶ç±»å‹ï¼Œä»æ–‡ä»¶ä¸­æå–çº¯æ–‡æœ¬å†…å®¹ã€‚
    æ”¯æŒçš„æ ¼å¼: .pdf, .docx, .doc, .xlsx, .xls
    Args:
        file_path (str|Path): æ–‡ä»¶è·¯å¾„ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœæ–‡ä»¶ç±»å‹ä¸æ”¯æŒæˆ–å‘ç”Ÿé”™è¯¯åˆ™è¿”å›é”™è¯¯ä¿¡æ¯
    """
    # è½¬æ¢ä¸º Path å¯¹è±¡ä»¥ç¡®ä¿ä¸€è‡´æ€§
    file_path = Path(file_path)
    extension = file_path.suffix.lower()
    
    try:
        if extension == '.pdf':
            doc = fitz.open(file_path)
            text = "".join(page.get_text() for page in doc)
            doc.close()
            return text

        elif extension == '.docx':
            doc = docx.Document(file_path)
            return "\n".join([para.text for para in doc.paragraphs])

        elif extension == '.doc':
            html = PyDocX.to_html(str(file_path))  # PyDocX å¯èƒ½éœ€è¦å­—ç¬¦ä¸²è·¯å¾„
            soup = BeautifulSoup(html, 'html.parser')
            return soup.get_text()

        elif extension == '.xlsx':
            text_parts = []
            workbook = openpyxl.load_workbook(file_path)
            for sheet in workbook.worksheets:
                for row in sheet.iter_rows():
                    for cell in row:
                        if cell.value:
                            text_parts.append(str(cell.value))
            return "\n".join(text_parts)

        elif extension == '.xls':
            text_parts = []
            workbook = xlrd.open_workbook(str(file_path))  # xlrd å¯èƒ½éœ€è¦å­—ç¬¦ä¸²è·¯å¾„
            for sheet in workbook.sheets():
                for row_idx in range(sheet.nrows):
                    for col_idx in range(sheet.ncols):
                        cell_value = sheet.cell_value(row_idx, col_idx)
                        if cell_value:
                            text_parts.append(str(cell_value))
            return "\n".join(text_parts)
            
        else:
            return f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {extension}"

    except Exception as e:
        logging.error(f"å¤„ç†æ–‡ä»¶ {file_path.name} æ—¶å‡ºé”™: {e}")
        return f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™"
# ...existing code...

def chat_chat_completion():
    """
    å¯¹openaiåº“çš„å†å°è£…,ä¾¿äºé¡¹ç›®ä½¿ç”¨
    """
    pass


if __name__ == "__main__":
    t = '''
    {
        "search_purpose": "æœç´¢åœ¨Linuxç¯å¢ƒä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨Debianç³»ç»Ÿä¸Šï¼Œå¦‚ä½•åœ¨RK3399å¹³å°ä½¿ç”¨QEMUçš„KVMåŠ é€ŸåŠŸèƒ½ã€‚é¢„æœŸå¾—åˆ°çš„å†…å®¹åŒ…æ‹¬ï¼šå…·ä½“çš„é…ç½®æ­¥éª¤ã€å®‰è£…å’Œè®¾ç½®KVMåŠ é€Ÿçš„æ•™ç¨‹ã€å¯èƒ½éœ€è¦çš„é¢å¤–é©±åŠ¨æˆ–å·¥å…·ã€é’ˆå¯¹RK3399ç¡¬ä»¶å¹³å°çš„ç‰¹æ®Šæ³¨æ„äº‹é¡¹æˆ–é™åˆ¶ã€ä»¥åŠç”¨æˆ·åœ¨å®é™…æ“ä½œä¸­å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›è¯¦ç»†ä¸”å¯æ“ä½œçš„æŒ‡å¯¼ï¼Œç¡®ä¿ä»–ä»¬èƒ½å¤Ÿåœ¨Debianç³»ç»Ÿä¸ŠæˆåŠŸå¯ç”¨QEMUçš„KVMåŠ é€Ÿã€‚",
        "time_page": [0, 0, 0],
        "data": [
            {
                "keys": "RK3399 QEMU KVM acceleration Linux Debian",
                "language": "en-US"
            },
            {
                "keys": "How to enable KVM on QEMU for RK3399 in Debian",
                "language": "en-US"
            },
            {
                "keys": "RK3399 QEMU KVM setup guide Debian Linux",
                "language": "en-US"
            }
        ]
    }
    '''
    logging.info(json2SearchRequests(json.loads(t)))
    # usage = {"completion_tokens": 16, "prompt_tokens": 4, "total_tokens": 20}
    # print(sse_create_openai_usage_data(usage))
