"""
å·¥å…·æ¨¡å—ï¼Œæä¾›JSONè§£æã€SSEæ•°æ®è½¬æ¢ã€æœç´¢è¯·æ±‚å¤„ç†ç­‰åŠŸèƒ½ã€‚
"""

import ast
import json
import re
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Union

from openai import OpenAI

# æ·»åŠ æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from app.search.models import SearchRequest,QueryKeys


def response2json(text: str, mode: str = "json_str") -> Union[Dict, List, None]:
    """
    ä»å­—ç¬¦ä¸²ä¸­æå–ç¬¬ä¸€ä¸ª { å’Œæœ€åä¸€ä¸ª } ä¹‹é—´çš„å†…å®¹ æˆ–è€… ç¬¬ä¸€ä¸ª [ å’Œ æœ€åä¸€ä¸ª ] ä¹‹é—´çš„å†…å®¹,
    å¹¶å°è¯•å°†å…¶è§£æä¸º JSON å¯¹è±¡æˆ–åˆ—è¡¨ã€‚
    ä¼šæ ¹æ® mode ä¼˜å…ˆå°è¯•è§£æã€‚å¦‚æœé¦–é€‰ç±»å‹æœªæ‰¾åˆ°ï¼Œä¼šå°è¯•å¦ä¸€ç§ç±»å‹ä½œä¸ºå›é€€ã€‚

    å‚æ•°:
        text (str): è¦å¤„ç†çš„è¾“å…¥å­—ç¬¦ä¸²
        mode (str): "json_str" (é»˜è®¤) ä¼˜å…ˆæŸ¥æ‰¾ JSON å¯¹è±¡ ({...}),
                    "json_list" ä¼˜å…ˆæŸ¥æ‰¾ JSON åˆ—è¡¨ ([...])ã€‚

    è¿”å›:
        dict/list/None: æˆåŠŸæ—¶è¿”å›è§£æåçš„ JSON å¯¹è±¡æˆ–åˆ—è¡¨, å¤±è´¥æ—¶è¿”å› None
    """
    text = re.sub(r'/\*.*?\*/', '', text, flags=re.DOTALL)
    
    pattern_json_obj = r"({.*})"  # åŒ¹é… JSON å¯¹è±¡: ç¬¬ä¸€ä¸ª { åˆ°æœ€åä¸€ä¸ª }
    pattern_json_list = r"(\[.*\])"  # åŒ¹é… JSON åˆ—è¡¨: ç¬¬ä¸€ä¸ª [ åˆ°æœ€åä¸€ä¸ª ]
    
    match_obj = re.search(pattern_json_obj, text, re.DOTALL)
    match_list = re.search(pattern_json_list, text, re.DOTALL)
    
    string_to_parse = None
    
    if mode == "json_list":
        if match_list:
            string_to_parse = match_list.group(1)
        elif match_obj:  # å›é€€: å¦‚æœåˆ—è¡¨æ¨¡å¼ä¸‹æœªæ‰¾åˆ°åˆ—è¡¨ï¼Œåˆ™å°è¯•å¯¹è±¡
            string_to_parse = match_obj.group(1)
    else:
        if match_obj:
            string_to_parse = match_obj.group(1)
        elif match_list:  # å›é€€: å¦‚æœå¯¹è±¡æ¨¡å¼ä¸‹æœªæ‰¾åˆ°å¯¹è±¡ï¼Œåˆ™å°è¯•åˆ—è¡¨
            string_to_parse = match_list.group(1)
    
    if string_to_parse:
        try:
            parsed_json_content = json.loads(string_to_parse)
            return parsed_json_content
        except json.JSONDecodeError as e:
            print(f"JSON è§£æå¤±è´¥: {e}")
            return None
    else:
        print("æœªæ‰¾åˆ°åŒ¹é…çš„ JSON æ ¼å¼å†…å®¹")
        return None


def response2list(llm_output: str) -> list[int]:
    """
    ä»ä»»æ„LLMè¾“å‡ºä¸­æå–æ•´æ•°åˆ—è¡¨
    
    Args:
        llm_output (str): å¤§æ¨¡å‹çš„è¾“å‡ºæ–‡æœ¬
        
    Returns:
        list: æå–å‡ºçš„æ•´æ•°åˆ—è¡¨ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨
    """
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åˆ—è¡¨æ ¼å¼
    llm_output = str(llm_output)
    list_pattern = r'\[\s*(?:\d+\s*(?:,\s*\d+\s*)*)?]'
    potential_lists = re.findall(list_pattern, llm_output)
    
    valid_lists = []
    for list_str in potential_lists:
        try:
            # ä½¿ç”¨ast.literal_evalå®‰å…¨åœ°è§£æåˆ—è¡¨
            parsed_list = ast.literal_eval(list_str)
            # ç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªåˆ—è¡¨å¹¶ä¸”æ‰€æœ‰å…ƒç´ éƒ½æ˜¯æ•´æ•°
            if isinstance(parsed_list, list) and all(isinstance(item, int) for item in parsed_list):
                valid_lists.append((list_str, parsed_list))
        except (SyntaxError, ValueError):
            continue
    
    if not valid_lists:
        return []
    
    # å¦‚æœåªæ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆåˆ—è¡¨ï¼Œç›´æ¥è¿”å›
    if len(valid_lists) == 1:
        return valid_lists[0][1]
    
    # å¦‚æœæœ‰å¤šä¸ªåˆ—è¡¨ï¼Œä¼˜å…ˆé€‰æ‹©æœ€é•¿çš„åˆ—è¡¨
    max_length = max(len(parsed_list) for _, parsed_list in valid_lists)
    longest_lists = [(list_str, parsed_list) for list_str, parsed_list in valid_lists 
                     if len(parsed_list) == max_length]
    
    # å¦‚æœæœ‰å¤šä¸ªç›¸åŒé•¿åº¦çš„åˆ—è¡¨ï¼Œé€‰æ‹©æœ€åå‡ºç°çš„é‚£ä¸ª
    last_list = None
    last_position = -1
    
    for list_str, parsed_list in longest_lists:
        list_pos = llm_output.rfind(list_str)
        if list_pos > last_position:
            last_position = list_pos
            last_list = parsed_list
    
    return last_list if last_list is not None else []


def get_time() -> str:
    """è·å–å½“å‰æ—¶é—´çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²"""
    return str(time.strftime("%Y-%m-%d %H:%M:%S", time.localtime()))


def sse_create_openai_data(content: str = '', reasoning_content: str = '') -> str:
    """åˆ›å»ºOpenAIæ ¼å¼çš„SSEæ•°æ®"""
    data_structure = {
        "choices": [
            {
                "delta": {
                    "content": str(content),
                    "reasoning_content": str(reasoning_content),
                    "role": "assistant"
                },
                "index": 0
            }
        ],
        "created": 0,  # æˆ–è€… int(time.time())
        "id": int(time.time()),
        "model": "summary_model",
        "service_tier": "default",
        "object": "chat.completion.chunk",
        "usage": None
    }
    json_data = json.dumps(data_structure, ensure_ascii=False)
    return f"data: {json_data}\n\n"


def sse_create_openai_usage_data(usage: dict) -> str:
    """åˆ›å»ºOpenAIæ ¼å¼çš„ä½¿ç”¨ç»Ÿè®¡SSEæ•°æ®"""
    completion_tokens = usage.get("completion_tokens", 0)
    prompt_tokens = usage.get("prompt_tokens", 0)
    total_tokens = usage.get("total_tokens", 0)
    template = f"""data: {{"usage":{{"completion_tokens":{completion_tokens},"prompt_tokens":{prompt_tokens},"total_tokens":{total_tokens}}},"choices":[],"created":{time.time()},"id":"search-{time.time()}","model":"search-model","object":"chat.completion.chunk"}}\n\n"""
    return template


def sse_gemini2openai_data(gemini_sse_data: str) -> str:
    """å°†Geminiçš„SSEæ•°æ®è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
    gemini_sse_data = gemini_sse_data.decode("utf-8")
    if gemini_sse_data.startswith("data: "):
        json_str = gemini_sse_data[6:]
        try:
            json_data = json.loads(json_str)
            content = json_data.get('candidates', [{}])[0].get('content', {}).get('parts', [{}])[0].get('text', '')
            data_structure = {
                "choices": [
                    {
                        "delta": {
                            "content": content,
                            "role": "assistant"
                        },
                        "index": 0
                    }
                ],
                "created": 0,
                "id": int(time.time()),
                "model": "summary_model", 
                "service_tier": "default",
                "object": "chat.completion.chunk",
                "usage": None
            }
            json_data = json.dumps(data_structure, ensure_ascii=False)
            return f"data: {json_data}\n\n"
        except Exception:
            pass
    else:
        print(gemini_sse_data)


def text2fc(llm_str: str) -> list:
    """ä»LLMè¾“å‡ºä¸­æå–å‡½æ•°è°ƒç”¨ä¿¡æ¯"""
    class fc:
        def __init__(self, name, arguments):
            self.function = self.function_llm(name, arguments)
            
        class function_llm:
            def __init__(self, name, arguments):
                self.name = name
                self.arguments = arguments
                
    pattern = r"<function_call>(.*?)</function_call>"
    matches = re.findall(pattern, llm_str, re.DOTALL)
    if matches:
        fc_text = matches[0].strip("\n")
        try:
            fc_json = json.loads(fc_text)
            fc_name = fc_json.get('name', None)
            fc_arguments = fc_json.get('arguments', None)
            fc_message = fc(fc_name, fc_arguments)
            return [fc_message]
        except Exception as e:
            print("function callæå–å‡ºé—®é¢˜")
            print(e)
            print(fc_text)


def json2SearchRequests(data: dict) -> SearchRequest:
    """
    å°†JSONæ•°æ®è½¬æ¢ä¸ºSearchRequestå¯¹è±¡åˆ—è¡¨
    
    å‚æ•°:
        data (dict): åŒ…å«æœç´¢è¯·æ±‚ä¿¡æ¯çš„å­—å…¸ï¼Œåº”åŒ…å«'search_purpose'ã€'data'å’Œ'time_page'å­—æ®µ
        
    è¿”å›:
        list[SearchRequest]: SearchRequestå¯¹è±¡åˆ—è¡¨
    """
    search_purpose = data.get('search_purpose', '')
    search_data = data.get('data', [])
    time_page = data.get('time_page', '')
    
    # éªŒè¯search_dataæ˜¯åˆ—è¡¨
    if not isinstance(search_data, list):
        return []
    
    search_request = SearchRequest(
            query_keys=[QueryKeys(key=r.get('keys', ''),language=r.get('language', 'zh_CN')) for r in search_data],
            time_page=time_page,
            search_purpose=search_purpose
        )
    
    return search_request


def format_search_plan(plan_info: dict):
    """
    ç¾åŒ–æœç´¢è®¡åˆ’è¾“å‡ºçš„ç”Ÿæˆå™¨å‡½æ•° - äºŒçº§èœå•æ ·å¼
    """
    search_purpose = plan_info.get('search_purpose', 'æœªçŸ¥')
    search_keywords = [item.get('keys', '') for item in plan_info.get('data', [])]
    
    yield f"ğŸ“Œ **æ–°çš„æœç´¢è®¡åˆ’ï¼š**\n"
    yield f"ğŸ¯ **æœç´¢ç›®çš„ï¼š** {search_purpose}\n"
    yield f"ğŸ” **æœç´¢å…³é”®è¯ï¼š**\n"
    
    for keyword in search_keywords:
        yield f"\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0\u00A0â¤ {keyword}\n"


def chat_chat_completion():
    """
    å¯¹openaiåº“çš„å†å°è£…,ä¾¿äºé¡¹ç›®ä½¿ç”¨
    """
    pass


if __name__ == "__main__":
    t = '''
    {
        "search_purpose": "æœç´¢åœ¨Linuxç¯å¢ƒä¸‹ï¼Œç‰¹åˆ«æ˜¯åœ¨Debianç³»ç»Ÿä¸Šï¼Œå¦‚ä½•åœ¨RK3399å¹³å°ä½¿ç”¨QEMUçš„KVMåŠ é€ŸåŠŸèƒ½ã€‚é¢„æœŸå¾—åˆ°çš„å†…å®¹åŒ…æ‹¬ï¼šå…·ä½“çš„é…ç½®æ­¥éª¤ã€å®‰è£…å’Œè®¾ç½®KVMåŠ é€Ÿçš„æ•™ç¨‹ã€å¯èƒ½éœ€è¦çš„é¢å¤–é©±åŠ¨æˆ–å·¥å…·ã€é’ˆå¯¹RK3399ç¡¬ä»¶å¹³å°çš„ç‰¹æ®Šæ³¨æ„äº‹é¡¹æˆ–é™åˆ¶ã€ä»¥åŠç”¨æˆ·åœ¨å®é™…æ“ä½œä¸­å¯èƒ½é‡åˆ°çš„å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚ç›®æ ‡æ˜¯ä¸ºç”¨æˆ·æä¾›è¯¦ç»†ä¸”å¯æ“ä½œçš„æŒ‡å¯¼ï¼Œç¡®ä¿ä»–ä»¬èƒ½å¤Ÿåœ¨Debianç³»ç»Ÿä¸ŠæˆåŠŸå¯ç”¨QEMUçš„KVMåŠ é€Ÿã€‚",
        "time_page": [0, 0, 0],
        "data": [
            {
                "keys": "RK3399 QEMU KVM acceleration Linux Debian",
                "language": "en-US"
            },
            {
                "keys": "How to enable KVM on QEMU for RK3399 in Debian",
                "language": "en-US"
            },
            {
                "keys": "RK3399 QEMU KVM setup guide Debian Linux",
                "language": "en-US"
            }
        ]
    }
    '''
    print(json2SearchRequests(json.loads(t)))
    # usage = {"completion_tokens": 16, "prompt_tokens": 4, "total_tokens": 20}
    # print(sse_create_openai_usage_data(usage))
