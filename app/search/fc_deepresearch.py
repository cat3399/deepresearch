import json
import sys
import time
import traceback
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path

from openai import OpenAI

# å°† ROOT_DIR çš„è§£æå’Œè·¯å¾„è¿½åŠ æ”¾åœ¨æ¨¡å—å¯¼å…¥çš„æ›´å‰é¢ï¼Œä»¥ç¡®ä¿è·¯å¾„è®¾ç½®å°½æ—©ç”Ÿæ•ˆ
ROOT_DIR = Path(__file__).resolve().parent.parent.parent
if str(ROOT_DIR) not in sys.path:
    sys.path.append(str(ROOT_DIR))

from app.search.fc_search import search_core
from app.search.models import SearchRequest, SearchResult, SearchResults
from app.search.search_after_ai import deepscan, is_duplicate
from app.search.search_searxng_api import search_api_worker
from app.utils.black_url import URL_BLACKLIST
from config.base_config import (SEARCH_API_LIMIT,
                              SEARCH_KEYWORD_API_KEY, SEARCH_KEYWORD_API_URL,SEARCH_KEYWORD_MODEL,
                              EVALUATE_API_KEY, EVALUATE_API_URL, EVALUATE_MODEL, EVALUATE_THREAD_NUM
                              )
from config.logging_config import logger
from app.utils.prompt import (DEEPRESEARCH_FIRST_PROMPT,
                              DEEPRESEARCH_NEXT_PROMPT, GET_VALUE_URL_PROMPT)
from app.utils.tools import (format_search_plan, format_urls, get_time,
                             json2SearchRequests, response2json)

client = OpenAI(api_key=SEARCH_KEYWORD_API_KEY, base_url=SEARCH_KEYWORD_API_URL)
# æ—¥å¿—é…ç½®åœ¨ config/logging_config.py ä¸­ç»Ÿä¸€ç®¡ç†


def _search_valuable_results(
    search_request: SearchRequest, excluded_urls: list[str] = None
) -> SearchResults:
    if excluded_urls is None:
        excluded_urls = [""]  # ä¿æŒåŸæœ‰é»˜è®¤è¡Œä¸º

    start_time = time.time()
    new_results = []
    if not search_request:
        logger.warning("æœç´¢è¯·æ±‚åˆ—è¡¨ä¸ºç©ºã€‚")
        return SearchResults()

    search_purpose = search_request.search_purpose
    time_page = search_request.time_page
    logger.info(f"å¼€å§‹æœç´¢ - ç›®çš„: {search_purpose}")

    with ThreadPoolExecutor(
        max_workers=SEARCH_API_LIMIT
    ) as executor:
        futures = []
        for data in search_request.query_keys:
            query = data.key
            language = data.language
            logger.info(
                f"æœç´¢å…³é”®è¯: {query}, è¯­è¨€: {language}, æ—¶é—´èŒƒå›´: {time_page}"
            )
            if query and language:
                futures.append(
                    executor.submit(search_api_worker, query, language, time_page)
                )

        for future in futures:
            try:
                results = future.result()
                for result in results:
                    if not is_duplicate(result, new_results):
                        new_results.append(result)
            except Exception as e:
                logger.error(f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}")
                logger.error(traceback.format_exc())

    if URL_BLACKLIST:
        unique_results = [
            result
            for result in new_results
            if not any(
                result.get("url", "").startswith(prefix) for prefix in URL_BLACKLIST
            )
        ]
    else:
        unique_results = new_results

    if excluded_urls and excluded_urls != [""]: # ç¡®ä¿ excluded_urls æœ‰å®é™…å†…å®¹
        stripped_excluded_set = {ex_url.strip() for ex_url in excluded_urls}
        unique_results = [
            result
            for result in unique_results
            if result.get("url", "").strip() not in stripped_excluded_set
        ]
    logger.info(f"æ’é™¤äº† {len(new_results)-len(unique_results)} ä¸ªç»“æœ")
    if not unique_results:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœç´¢ç»“æœ")
        # è¿”å›ä¸€ä¸ªç©ºçš„ SearchResults å¯¹è±¡è€Œä¸æ˜¯å­—å…¸
        return SearchResults(search_request=search_request, results=[])

    search_results_objects = [
        SearchResult(
            url=result.get("url", ""),
            title=result.get("title", ""),
            content=result.get("content", ""),
        )
        for result in unique_results
    ]
    return SearchResults(
        search_request=search_request,
        results=search_results_objects,
    )


def generate_search_plan(messages: list[dict], web_reference: str = "", previous_plan: str = "", previous_results: str = "", max_remaining_steps: int = 8) -> list:
    """
    ç”Ÿæˆæœç´¢è®¡åˆ’
    
    Args:
        messages: ç”¨æˆ·æ¶ˆæ¯
        web_reference: åˆå§‹æœç´¢å‚è€ƒç»“æœ
        previous_plan: ä¸Šä¸€è½®æœç´¢è®¡åˆ’
        previous_results: ä¸Šä¸€è½®æœç´¢ç»“æœ
        max_remaining_steps: å‰©ä½™å¯æ‰§è¡Œæ­¥éª¤æ•°
    
    Returns:
        list: æœç´¢è®¡åˆ’æ­¥éª¤åˆ—è¡¨
    """
    if not previous_plan:
        # ç”Ÿæˆç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’
        prompt = DEEPRESEARCH_FIRST_PROMPT.substitute(
            current_time=get_time(),
            messages=str(messages),
            web_reference=web_reference
        )
    else:
        # ç”Ÿæˆåç»­æœç´¢è®¡åˆ’
        prompt = DEEPRESEARCH_NEXT_PROMPT.substitute(
            current_time=get_time(),
            messages=str(messages),
            max_step=max_remaining_steps,
            previous_search_plan=previous_plan,
            previous_search_results=previous_results,
        )
    # print(prompt)
        # print("previous_plan",previous_plan)
    try:
        llm_rsp = client.chat.completions.create(
            model=SEARCH_KEYWORD_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.1,
            stream=False,
        )
        llm_rsp_content = llm_rsp.choices[0].message.content
        
        try:
            completion_tokens = llm_rsp.usage.completion_tokens
            total_tokens = llm_rsp.usage.total_tokens
            logger.debug(f"æœç´¢è®¡åˆ’ç”Ÿæˆè¾“å‡ºtokens: {completion_tokens}")
            logger.debug(f"æœç´¢è®¡åˆ’ç”Ÿæˆæ€»å…±èŠ±è´¹tokens: {total_tokens}")
        except AttributeError:
            logger.debug("æ— æ³•è·å–æœç´¢è®¡åˆ’ç”Ÿæˆçš„ token ä½¿ç”¨æƒ…å†µã€‚")
        
        parsed_rsp = response2json(llm_rsp_content)
        steps = parsed_rsp.get("steps", [])
        return steps
    except Exception as e:
        logger.error(f"ç”Ÿæˆæœç´¢è®¡åˆ’æ—¶å‡ºé”™: {str(e)}")
        return []


def _execute_search_plan(search_plan_step: dict, excluded_urls: list[str] = None) -> SearchResults:
    """
    æ‰§è¡Œå•ä¸ªæœç´¢è®¡åˆ’æ­¥éª¤
    
    Args:
        search_plan_step: æœç´¢è®¡åˆ’æ­¥éª¤
        excluded_urls: æ’é™¤çš„URLåˆ—è¡¨
    
    Returns:
        SearchResults: æœç´¢ç»“æœ
    """
    if excluded_urls is None:
        excluded_urls = [""]
    logger.info(f"æ‰§è¡Œæœç´¢è®¡åˆ’: {search_plan_step}")
    search_request = json2SearchRequests(search_plan_step)
    if not search_request:
        logger.warning("æœªèƒ½ä»è®¡åˆ’ä¸­è§£æå‡ºæœ‰æ•ˆçš„æœç´¢è¯·æ±‚ã€‚")
        return SearchResults()

    search_results = _search_valuable_results(
        search_request=search_request, excluded_urls=excluded_urls
    )
    if not search_results.results:
        logger.warning("æœªèƒ½ä»æœç´¢ä¸­è·å–åˆ°ç»“æœã€‚")
        return SearchResults(search_request=search_request, results=[])

    # ç­›é€‰æœ‰ä»·å€¼çš„URL
    max_valuable_urls = max(1, int(search_request.max_search_results / 2))
    value_url_prompt = GET_VALUE_URL_PROMPT.substitute(
        current_time=get_time(),
        search_results=search_results.to_str(),
        search_purpose=search_request.search_purpose,
        max_num=max_valuable_urls,
        search_restrictions=search_request.search_restrictions
    )
    # print("value_url_prompt: ",value_url_prompt)
    try:
        client = OpenAI(api_key=EVALUATE_API_KEY, base_url=EVALUATE_API_URL)
        llm_rsp_value = client.chat.completions.create(
            model=EVALUATE_MODEL,
            messages=[{"role": "user", "content": value_url_prompt}],
            temperature=0.1,
            stream=False,
        )

        try:
            completion_tokens_val = llm_rsp_value.usage.completion_tokens
            total_tokens_val = llm_rsp_value.usage.total_tokens
            logger.debug(f"ä»·å€¼URLç­›é€‰è¾“å‡ºtokens: {completion_tokens_val}")
            logger.debug(f"ä»·å€¼URLç­›é€‰æ€»å…±èŠ±è´¹tokens: {total_tokens_val}")
        except AttributeError:
            logger.debug("æ— æ³•è·å–ä»·å€¼URLç­›é€‰çš„ token ä½¿ç”¨æƒ…å†µã€‚")

        llm_rsp_content_value = llm_rsp_value.choices[0].message.content
        logger.debug(f"LLMè¿”å›çš„ä»·å€¼URLç¼–å·: {llm_rsp_content_value}")
        url_num_list_json = response2json(llm_rsp_content_value)
        valuable_results_data = []
        if url_num_list_json and isinstance(url_num_list_json, list):
            search_results_dict = search_results.to_dict()
            for url_num_str in url_num_list_json:
                key = f"ç½‘é¡µ{url_num_str}"
                if key in search_results_dict:
                    valuable_results_data.append(search_results_dict[key])
                else:
                    logger.warning(f"æœªèƒ½ä»æœç´¢ç»“æœå­—å…¸ä¸­æ‰¾åˆ°é”®: {key}")
        else:
            logger.warning(f"LLMè¿”å›çš„ä»·å€¼URLç¼–å·ä¸æ˜¯åˆ—è¡¨æ ¼å¼: {url_num_list_json}")

        if not valuable_results_data:
            logger.warning("æœªèƒ½ä»LLMå“åº”ä¸­æå–åˆ°æœ‰ä»·å€¼çš„URLç»“æœã€‚")
            return SearchResults(search_request=search_request)
        logger.debug(f"æœ‰ä»·å€¼çš„æœç´¢ç»“æœ: {valuable_results_data}")
        if valuable_results_data:
            search_plan_result = deepscan(
                search_response=valuable_results_data, search_request=search_request
            )
            return search_plan_result
        else:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°æœ‰ä»·å€¼çš„æœç´¢ç»“æœã€‚")
            return SearchResults(search_request=search_request)
    except Exception as e:
        logger.error(f"æ‰§è¡Œæœç´¢è®¡åˆ’æ—¶å‡ºé”™: {str(e)}")
        return SearchResults(search_request=search_request)

def deepresearch_tool(messages: list[dict]):
    executed_search_plans = []
    max_plan_iterations = 12
    accumulated_search_results = SearchResults()
    yield "ğŸ” **å¼€å§‹æ·±åº¦ç ”ç©¶æœç´¢...**\n\n"

    # æ‰§è¡Œåˆå§‹æœç´¢è·å–å‚è€ƒä¿¡æ¯
    yield "ğŸ“‹ **åˆå§‹æœç´¢è·å–å‚è€ƒä¿¡æ¯**\n"
    search_reference_results = search_core(messages=str(messages), deep=False)
    yield "âœ… åˆå§‹æœç´¢å®Œæˆ\n\n"

    # ç”Ÿæˆç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’
    yield "ğŸ“Œ **ç”Ÿæˆç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’**\n"
    current_search_plan_steps = generate_search_plan(
        messages=messages,
        web_reference=search_reference_results.to_str() if search_reference_results else ""
    )
    
    if not current_search_plan_steps:
        yield "âš ï¸ ç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’æœªèƒ½ç”Ÿæˆã€‚\n\n"
        yield "ğŸ **æ·±åº¦ç ”ç©¶ç»“æŸ**\n\n"
        yield f"results{accumulated_search_results.to_str()}"
        return

    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’
    executed_plan_item = current_search_plan_steps[0]
    yield from format_search_plan(executed_plan_item)
    
    # æ‰§è¡Œç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’
    yield "ğŸ”„ **æ‰§è¡Œç¬¬ä¸€ä¸ªæœç´¢è®¡åˆ’**\n"
    current_results = _execute_search_plan(executed_plan_item)
    logger.debug(f"å½“å‰æœç´¢ç»“æœ: {current_results.to_str()}")
    yield from format_urls(current_results.get_urls())
    executed_search_plans.append(executed_plan_item)
    accumulated_search_results.merge(current_results)
    excluded_urls = accumulated_search_results.get_urls()
    yield "âœ… æœç´¢è®¡åˆ’1æ‰§è¡Œå®Œæˆ\n\n"

    plan_counter = 2
    
    # ç»§ç»­ç”Ÿæˆå’Œæ‰§è¡Œåç»­æœç´¢è®¡åˆ’
    try:
        while len(executed_search_plans) < max_plan_iterations:
            yield f"ğŸ“Š **å·²æ‰§è¡Œçš„æœç´¢è®¡åˆ’æ•°é‡ï¼š** {len(executed_search_plans)}/{max_plan_iterations}\n"
            # ç”Ÿæˆä¸‹ä¸€ä¸ªæœç´¢è®¡åˆ’
            yield f"ğŸ“Œ **æ­¥éª¤{plan_counter}ï¼šç”Ÿæˆä¸‹ä¸€ä¸ªæœç´¢è®¡åˆ’**\n"
            current_search_plan_steps = generate_search_plan(
                messages=messages,
                previous_plan=str([plan.get("search_purpose",'') for plan in executed_search_plans]),
                previous_results=accumulated_search_results.to_str(),
                max_remaining_steps=max_plan_iterations - len(executed_search_plans)
            )

            if not current_search_plan_steps or len(current_search_plan_steps) == 0:
                yield "ğŸ **æœªèƒ½ç”Ÿæˆæ–°çš„æœç´¢è®¡åˆ’ï¼Œä¿¡æ¯è·å–å®Œæ¯•ï¼Œæ·±åº¦ç ”ç©¶æå‰å®Œæˆ**\n\n"
                break

            # æ˜¾ç¤ºæœç´¢è®¡åˆ’
            executed_plan_item = current_search_plan_steps[0]
            yield from format_search_plan(executed_plan_item)
            
            # æ‰§è¡Œæœç´¢è®¡åˆ’
            yield f"ğŸ”„ **æ‰§è¡Œæœç´¢è®¡åˆ’{plan_counter}**\n"
            current_results = _execute_search_plan(executed_plan_item,excluded_urls=excluded_urls)
            yield from format_urls(current_results.get_urls())
            accumulated_search_results.merge(current_results)
            excluded_urls += accumulated_search_results.get_urls()
            executed_search_plans.append(executed_plan_item)
            yield f"âœ… æœç´¢è®¡åˆ’{plan_counter}æ‰§è¡Œå®Œæˆ\n\n"
            plan_counter += 1

        # å¤„ç†å¾ªç¯ç»“æŸåçš„æƒ…å†µ
        if len(executed_search_plans) >= max_plan_iterations:
            yield f"ğŸ **å·²è¾¾åˆ°æœ€å¤§æœç´¢è®¡åˆ’æ•°é‡({max_plan_iterations})ï¼Œæ·±åº¦ç ”ç©¶å®Œæˆ**\n\n"
        elif not executed_search_plans:
            yield "ğŸ **æœªèƒ½æ‰§è¡Œä»»ä½•æœç´¢è®¡åˆ’ï¼Œæ·±åº¦ç ”ç©¶ç»“æŸ**\n\n"
        else:
            yield "ğŸ **æ·±åº¦ç ”ç©¶å®Œæˆ**\n\n"

        yield "âœ… **æ·±åº¦ç ”ç©¶æœç´¢å®Œæˆ**\n"
    except:
        traceback.print_exc()
        yield "ğŸš« **ç ”ç©¶è¿‡ç¨‹å‡ºç°æ„å¤–,å¼ºè¡Œç»ˆæ­¢**"
    
    # å°†ç»“æœå†™å…¥æ–‡ä»¶
    try:
        with open("temp_research.txt", "w", encoding="utf-8") as fp:
            fp.write(accumulated_search_results.to_str())
        yield f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° temp_research.txt\n"
    except IOError as e:
        logger.error(f"å†™å…¥ temp_research.txt æ–‡ä»¶å¤±è´¥: {e}")
        yield f"âš ï¸ æ— æ³•å†™å…¥ç»“æœæ–‡ä»¶: {e}\n"

    yield f"results{accumulated_search_results.to_str()}"



if __name__ == "__main__":
    user_messages_example = [{"role": "user", "content": "ä»‹ç»ä¸€ä¸‹chatgpt o4mini"}]
    # print(f"æ­£åœ¨å¯¹ \"{user_messages_example[0]['content']}\" è¿›è¡Œæ·±åº¦ç ”ç©¶...")
    for item in deepresearch_tool(user_messages_example):
        logger.info(item)
    logger.info("--- ç ”ç©¶ç»“æŸ ---")
